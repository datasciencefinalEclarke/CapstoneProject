---
title: "capstone_model_2 (lr)"
output: pdf_document
date: "2022-12-17"
---


```{r}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)      # for data wrangling
library(ggplot2)    # for awesome graphics
library(rsample)    # for creating validation splits
library(recipes)    # for feature engineering
library(dslabs)     # for mnist data
library(purrr)      #for mapping
library(tidyverse)  # for filtering 
library(modeldata)
library(ggpubr)
library(caret)       # for fitting KNN models
library(MASS)
library(caTools)
library(pROC)
library(vip)
library(ROCR)
```
#import dataset
```{r}
radiomics <- read.csv("radiomics_completedata.csv")

str(radiomics)
glimpse(radiomics)

```


# initial dimension and check for missing values
```{r}
dim(radiomics)

is.na(radiomics)
sum(is.na(radiomics))
na.omit(radiomics)

```
#normality test
```{r}
attach(radiomics)
summary(Entropy_cooc.W.ADC)
summary(GLNU_align.H.PET)
summary(Energy_hist.PET)
hist(Entropy_cooc.W.ADC)
hist(GLNU_align.H.PET)
hist(Energy_hist.PET)
```

#convert to matrix and test normality with QQ-plot

```{r} 

dummy <- dummyVars(Failure.binary ~ ., data=radiomics)
newdata <- data.frame(predict(dummy, newdata = radiomics)) 
newdata
newdata2 <- as.matrix(newdata)
qqnorm(newdata2)
```


#normality graphs 
```{r}
boxplot(Entropy_cooc.W.ADC, main = "Entropy_cooc.W.ADC")
hist(Entropy_cooc.W.ADC)
```
#correlation
```{r}
newdf1 = subset(radiomics, select = c(-Institution))
newdf1
cor.newdf1 = cor(newdf1)
corr = round(cor.newdf1,2)  
```
# Create training (80%) and test (20%) sets 
```{r}
df <- radiomics %>% mutate_if(is.ordered, factor, ordered = FALSE)

set.seed(123)   
churn_split <- initial_split(df, prop = 0.8, strata = "Failure.binary")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
```
#Model training
```{r}
set.seed(123)
cv_model1 <- train(
  Failure.binary ~ Entropy_area.W.ADC, 
  data = churn_train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

set.seed(123)
cv_model2 <- train(
  Failure.binary ~ Entropy_area.W.ADC + GLNU_norm_align.W.ADC, 
  data = churn_train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

set.seed(123)
cv_model3 <- train(
  Failure.binary ~ ., 
  data = churn_train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)
```
# extract out of sample performance measures
```{r}
summary(
  resamples(
    list(
      model1 = cv_model1, 
      model2 = cv_model2, 
      model3 = cv_model3
    )
  )
)$statistics$Accuracy

```


# predict class
```{r}

pred_class_1 <- predict(cv_model1, churn_train)
```

# create confusion matrix
```{r, eval = FALSE}
confusionMatrix(
  data = relevel(pred_class_1, ref = "Yes"), 
  reference = relevel(churn_train$Failure.binary, ref = "Yes")
)


pred_class_2 <- predict(cv_model2, churn_train)
```

# create confusion matrix
```{r, eval = FALSE}

confusionMatrix(
  data = relevel(pred_class_2, ref = "Yes"), 
  reference = relevel(churn_train$Failure.binary, ref = "Yes")
)

pred_class_3 <- predict(cv_model3, churn_train)
```

# create confusion matrix
```{r, eval = FALSE}
confusionMatrix(
  data = relevel(pred_class_3, ref = "Yes"), 
  reference = relevel(churn_train$Failure.binary, ref = "Yes")
)
```


# Compute predicted probabilities on training data
```{r, eval = FALSE}
m1_prob <- predict(cv_model1, churn_train, type = "prob")$Yes
m2_prob <- predict(cv_model2, churn_train, type = "prob")$Yes
m3_prob <- predict(cv_model3, churn_train, type = "prob")$Yes
```
# Compute AUC metrics for cv_model1,2 and 3  (training phase)
```{r, eval = FALSE}
perf1 <- prediction(m1_prob, churn_train$Failure.binary) %>%
  performance(measure = "tpr", x.measure = "fpr")
perf2 <- prediction(m2_prob, churn_train$Failure.binary) %>%
  performance(measure = "tpr", x.measure = "fpr")
perf3 <- prediction(m3_prob, churn_train$Failure.binary) %>%
  performance(measure = "tpr", x.measure = "fpr")
```
# Plot ROC curves for cv_model1,2 and 3 
```{r, eval = FALSE}
plot(perf1, col = "black", lty = 2)
plot(perf2,  add = TRUE, col = "red", lty = 2)
plot(perf3, add = TRUE, col = "blue")
legend(0.7, 0.3, legend = c("cv_model1", "cv_model2", "cv_model3"),
       col = c("black","red", "blue"), lty = 3:1, cex = 0.6)
```
# ROC plot for training data
```{r, eval = FALSE}
roc(churn_train$Failure.binary ~ m1_prob, plot=TRUE, legacy.axes=FALSE, 
    percent=TRUE, col="black", lwd=2, print.auc=TRUE)
plot.roc(churn_train$Failure.binary ~ m2_prob,  percent=TRUE, col="red", 
         lwd=2, print.auc=TRUE, add=TRUE, print.auc.y=40)
plot.roc(churn_train$Failure.binary ~ m3_prob,  percent=TRUE, col="blue", 
         lwd=2, print.auc=TRUE, add=TRUE, print.auc.y=30)
title(main = "Model Performance during Training", line = 2.5)
```

#Feature Interpretation
```{r, eval = FALSE}
vip(cv_model3, num_features = 20)
```
#top 20 important features during training
```{r, eval = FALSE}
vi <- varImp(cv_model3)
vi
```
# Compute predicted probabilities on test data
```{r, eval = FALSE}
m1_prob <- predict(cv_model1, churn_test, type = "prob")$Yes
m2_prob <- predict(cv_model2, churn_test, type = "prob")$Yes
m3_prob <- predict(cv_model3, churn_test, type = "prob")$Yes
```
# Compute AUC metrics for cv_model1,2 and 3 (testing phase)
```{r, eval = FALSE}
perf1 <- prediction(m1_prob, churn_test$Failure.binary) %>%
  performance(measure = "tpr", x.measure = "fpr")
perf2 <- prediction(m2_prob, churn_test$Failure.binary) %>%
  performance(measure = "tpr", x.measure = "fpr")
perf3 <- prediction(m3_prob, churn_test$Failure.binary) %>%
  performance(measure = "tpr", x.measure = "fpr")
```
# Plot ROC curves for cv_model1,2 and 3 
```{r, eval = FALSE}
plot(perf1, col = "black", print.auc=TRUE, lty = 2)
plot(perf2,  add = TRUE, col = "red",  print.auc=TRUE, lty = 2)
plot(perf3, add = TRUE, col = "blue", print.auc=TRUE)
legend(0.7, 0.3, legend = c("cv_model1", "cv_model2", "cv_model3"),
       col = c("black","red", "blue"), lty = 3:1, cex = 0.6)
```
# ROC plot for testing data
```{r, eval = FALSE}
roc(churn_test$Failure.binary ~ m1_prob, plot=TRUE, legacy.axes=FALSE, 
    percent=TRUE, col="black", lwd=2, print.auc=TRUE)
plot.roc(churn_test$Failure.binary ~ m2_prob,  percent=TRUE, col="red", 
         lwd=2, print.auc=TRUE, add=TRUE, print.auc.y=40)
plot.roc(churn_test$Failure.binary ~ m3_prob,  percent=TRUE, col="blue", 
         lwd=2, print.auc=TRUE, add=TRUE, print.auc.y=30)
title(main = "Model Performance during Testing", line = 2.5)
```

 